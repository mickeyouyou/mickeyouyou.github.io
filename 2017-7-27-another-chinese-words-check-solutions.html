<html>
<head>
	
	<!-- hexo-inject:begin --><!-- hexo-inject:end --><title>汉语错别字检查、词组建议的一个方案</title>
	<meta name="keywords" content="fzb.me,冯宗宝,冯宗宝的blog" />

    <meta http-equiv="content-type" content="text/html; charset=utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

    
	   <link href="/css/main.css?v=3" rel="stylesheet" type="text/css" />
    
        <script src="/js/util.js" type="text/javascript"></script>
        <script>
            if(isMobile()) {
                loadjscssfile('../css/mobile.css', 'css');
            } else {
                loadjscssfile('../css/desktop.css', 'css');
            }
        </script> 
    

    <link rel="alternate" type="application/atom+xml" href="/atom.xml" title="Atom feed">

    
	<link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=3"/><!-- hexo-inject:begin --><!-- hexo-inject:end -->
    

</head>

<body>


<!-- hexo-inject:begin --><!-- hexo-inject:end --><h2 class="title">汉语错别字检查、词组建议的一个方案</h2>
<!---
<div style="text-align:center;margin-top: -10px;">
<div class="article-category">
发表于2017年7月27日




 </div>
--->
</div>

<ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#背景"><span class="toc-text">背景</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#汉字的数字化"><span class="toc-text">汉字的数字化</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#实数向量-One-hot-Representation"><span class="toc-text">实数向量 One-hot Representation</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#词向量-Distributed-Representation"><span class="toc-text">词向量 Distributed Representation</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#模型"><span class="toc-text">模型</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#语言模型"><span class="toc-text">语言模型</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#N-gram-neural-model"><span class="toc-text">N-gram neural model</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Continuous-Bag-of-Words-model-CBOW"><span class="toc-text">Continuous Bag-of-Words model(CBOW)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Skip-gram-model"><span class="toc-text">Skip-gram model</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#PaddlePaddle"><span class="toc-text">PaddlePaddle</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Paddle-Book"><span class="toc-text">Paddle Book</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Paddle-Models"><span class="toc-text">Paddle Models</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#语言处理"><span class="toc-text">语言处理</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#如何判断词组的搭配合理性？"><span class="toc-text">如何判断词组的搭配合理性？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#学习过程"><span class="toc-text">学习过程</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#交互流程"><span class="toc-text">交互流程</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#覆盖的端"><span class="toc-text">覆盖的端</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#参考"><span class="toc-text">参考</span></a></li></ol>
<blockquote>
<p>本文针对的是汉语错别字检查，或者为写作者提供建议词汇组合的一个方案。</p>
</blockquote>
<h3 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h3><p>故事的背景，从新华社的一篇不论是浏览量还是评论量奇高的新闻稿件（<a href="http://mp.weixin.qq.com/s/75-QKDcu-0GWOKOP7iAkOg" target="_blank" rel="external">公众号文章</a>）说起。</p>
<p><img src="http://7tsys1.com1.z0.glb.clouddn.com/%E5%BA%9F%E9%BB%9C%EF%BC%88%E5%BA%9F%E9%99%A4%EF%BC%89.jpg" width="70%" alt="废黜与废除"><br>$$图1,搜狗输入法发布的“权威错别字”$$</p>
<p>我们假定新华社这么多位编辑想输入的是“废黜”，其实他们也确实想这么做，用机器学习的方式指出他们的错误之处，我们还需要知道“废黜”跟“废除”的相关性。</p>
<blockquote>
<p>汉语词语正确性，都是相对的正确性。汉语词组从语境中独立出来看，没有绝对的错误。所以讨论就应该是这个词在当前句子中的正确性。</p>
</blockquote>
<h3 id="汉字的数字化"><a href="#汉字的数字化" class="headerlink" title="汉字的数字化"></a>汉字的数字化</h3><p>为了做这样的比较，我们往往先要把词表示成计算机适合处理的方式。即自然语言理解的问题转化为机器学习的问题，第一步把这些符号数字化。</p>
<h4 id="实数向量-One-hot-Representation"><a href="#实数向量-One-hot-Representation" class="headerlink" title="实数向量 One-hot Representation"></a>实数向量 One-hot Representation</h4><p>NLP 中最直观，也是到目前为止最常用的词表示方法是 One-hot Representation，把每个词表示为一个很长的向量。这个向量的维度是词表大小，其中绝大多数元素为 0，只有一个维度的值为 1，这个维度就代表了当前的词。</p>
<p>示例：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#27801;&#29305;&#22269;&#29579;&#33832;&#21202;&#26364;21&#26085;&#23459;&#24067;&#65292;&#24223;&#38500;&#29579;&#20648;&#40664;&#32597;&#40664;&#24503;&#183;&#26412;&#183;&#32435;&#20234;&#22827;&#65292;&#21478;&#31435;&#31302;&#32597;&#40664;&#24503;&#183;&#26412;&#183;&#33832;&#21202;&#26364;&#20026;&#26032;&#20219;&#29579;&#20648;&#12290;</span><br></pre></td></tr></table></figure></p>
<p>分词结果，由23个词组成：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#27801;&#29305; &#22269;&#29579; &#33832;&#21202;&#26364; 21&#26085; &#23459;&#24067; &#24223;&#38500; &#29579;&#20648; &#31302;&#32597;&#40664;&#24503; &#183;  &#26412; &#183; &#32435;&#20234;&#22827; , &#21478;&#31435; &#31302;&#32597;&#40664;&#24503; &#183; &#26412; &#183; &#33832;&#21202;&#26364; &#20026; &#26032;&#20219; &#29579;&#20648; &#12290;</span><br></pre></td></tr></table></figure></p>
<p>如下列出了上面两个词的编码为：</p>
<ul>
<li>废除 = [0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 …]</li>
<li>废黜 = [0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 …]</li>
</ul>
<p>理论上将，这种编辑都容易混淆的词，必然被人意识上相似性扰乱，从而影响词在句子中的正确性。然而这两个词对应的One-hot Representations之间的距离度量，无论是欧式距离还是余弦相似度，由于其向量正交，都认为这两个词毫无相关性。</p>
<p>这就是这种表示方法存在一个重要问题：“<code>词汇鸿沟</code>”现象：任意两个词之间都是孤立的，每个词本身的信息量都太小，光从这两个向量中看不出两个词是否有关系。要想精确计算相关性，我们还需要更多的信息——从大量数据里通过机器学习方法归纳出来的知识</p>
<h4 id="词向量-Distributed-Representation"><a href="#词向量-Distributed-Representation" class="headerlink" title="词向量 Distributed Representation"></a>词向量 Distributed Representation</h4><p>在机器学习领域里，各种“知识”被各种模型表示，词向量模型(word embedding model)就是其中的一类。通过词向量模型可将一个 One-hot Representation映射到一个维度更低的实数向量（embedding vector），在这个映射到的实数向量表示中，希望两个语义（或用法）上相似的词对应的词向量“更像”，如废除的废黜的词向量表示：</p>
<ul>
<li>废除 = [0.347811, 0.419389, -0.507915, 0.0933927, …]</li>
<li>废黜 = [0.153066, 0.320302, 0.0317544, 0.106635, …]</li>
</ul>
<p>这样如“废除”和“废黜”的对应词向量的余弦相似度就不再为零了。<code>废除</code>与<code>废黜</code>的相似性评估得分：<br><figure class="highlight php"><table><tr><td class="code"><pre><span class="line"><span class="string">'score'</span> =&gt; float <span class="number">0.21303</span></span><br><span class="line"><span class="string">'words'</span> =&gt; </span><br><span class="line">  <span class="keyword">array</span> (size=<span class="number">2</span>)</span><br><span class="line">    <span class="string">'word_2'</span> =&gt; string <span class="string">'废黜'</span> (length=<span class="number">6</span>)</span><br><span class="line">    <span class="string">'word_1'</span> =&gt; string <span class="string">'废除'</span> (length=<span class="number">6</span>)</span><br></pre></td></tr></table></figure></p>
<p>可以看到这两个词的相似性概率，并不算太高。</p>
<p>词向量模型可以是概率模型或神经元网络模型。</p>
<p>此处引用神经网络模型。</p>
<p>基于神经网络的模型不需要计算存储一个在全语料上统计的大表，而是通过学习语义信息得到词向量，因此能很好地解决以上问题。语义信息到词向量的学习过程使用PaddlePaddle完成。</p>
<p>统计学习的三要素：方法 = 模型 + 策略 + 算法</p>
<h3 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h3><h4 id="语言模型"><a href="#语言模型" class="headerlink" title="语言模型"></a>语言模型</h4><p>在介绍词向量模型之前，我们先来引入一个概念：语言模型。 语言模型旨在为语句的联合概率函数$P(w_1, …, w_T)$建模, 其中$w_i$表示句子中的第i个词。语言模型的目标是，希望模型对有意义的句子赋予大概率，对没意义的句子赋予小概率。 这样的模型可以应用于很多领域，如机器翻译、语音识别、信息检索、词性标注、手写识别等，它们都希望能得到一个连续序列的概率。 </p>
<p>以上面的稿件为例，当编辑输入“<code>沙特国王萨勒曼21日宣布，废除王储默罕默德·本·纳伊夫，另立穆罕默德·本·萨勒曼为新任王储。</code>”时，识别引擎至少会提示你是否希望输入”<code>沙特国王萨勒曼21日宣布，废黜王储默罕默德·本·纳伊夫，另立穆罕默德·本·萨勒曼为新任王储。</code>“才是正确的, 因为根据语言模型计算出“<code>废除王储</code>”的概率很低，而与”<code>废黜</code>“近似的，可能引起错误的词中，”<code>废除</code>“会使该句生成的概率最大。</p>
<p>对语言模型的目标概率$P(w_1, …, w_T)$，如果假设文本中每个词都是相互独立的，则整句话的联合概率可以表示为其中所有词语条件概率的乘积，即：<br><span>$$\\
P(w_1, &hellip;, w_T) = \prod_{t=1}^TP(w_t)$$</span><!-- Has MathJax --><br>然而我们知道语句中的每个词出现的概率都与其前面的词紧密相关, 所以实际上通常用条件概率表示语言模型：<br><span>$$\\
P(w_1, &hellip;, w_T) = \prod_{t=1}^TP(w_t | w_1, &hellip; , w_{t-1})$$</span><!-- Has MathJax --></p>
<h4 id="N-gram-neural-model"><a href="#N-gram-neural-model" class="headerlink" title="N-gram neural model"></a>N-gram neural model</h4><p>在计算语言学中，n-gram是一种重要的文本表示方法，表示一个文本中连续的n个项。基于具体的应用场景，每一项可以是一个字母、单词或者音节。 n-gram模型也是统计语言模型中的一种重要方法，用n-gram训练语言模型时，一般用每个n-gram的历史n-1个词语组成的内容来预测第n个词。</p>
<p>Yoshua Bengio等科学家的著名论文 <a href="http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf" target="_blank" rel="external">Neural Probabilistic Language Models</a> 中介绍如何学习一个神经元网络表示的词向量模型。文中的神经概率语言模型（Neural Network Language Model，NNLM）通过一个线性映射和一个非线性隐层连接，同时学习了语言模型和词向量，即通过学习大量语料得到词语的向量表达，通过这些向量得到整个句子的概率。用这种方法学习语言模型可以克服维度灾难（curse of dimensionality）,即训练和测试数据不同导致的模型不准。</p>
<p>我们在上文中已经讲到用条件概率建模语言模型，即一句话中第t个词的概率和该句话的前t−1个词相关。可实际上越远的词语其实对该词的影响越小，那么如果考虑一个n-gram, 每个词都只受其前面n-1个词的影响，则有：<br><span>$$\\
P(w_1, &hellip;, w_T) = \prod_{t=n}^TP(w_t|w_{t-1}, w_{t-2}, &hellip;, w_{t-n+1})$$</span><!-- Has MathJax --><br>给定一些真实语料，这些语料中都是有意义的句子，N-gram模型的优化目标则是最大化目标函数:<br><span>$$\\
\frac{1}{T}\sum_t f(w_t, w_{t-1}, &hellip;, w_{t-n+1};\theta) + R(\theta)$$</span><!-- Has MathJax --></p>
<p>其中$f(w_t, w_{t-1}, …, w_{t-n+1})$表示根据历史n-1个词得到当前词$w_t$的条件概率，R(θ)表示参数正则项。</p>
<p><img src="http://book.paddlepaddle.org/04.word2vec/image/nnlm.png" width="99%" alt="N-gram神经网络模型"><br>$$图2,N-gram神经网络模型$$</p>
<p>上图展示了N-gram神经网络模型，从下往上看，该模型分为以下几个部分：</p>
<ul>
<li>对于每个样本，模型输入$w_{t-n+1},…w_{t-1}$, 输出句子第t个词为字典中|V|个词的概率。每个输入词$w_{t-n+1},…w_{t-1}$首先通过映射矩阵映射到词向量$C(w_{t-n+1}),…C(w_{t-1})$。</li>
<li>然后所有词语的词向量连接成一个大向量，并经过一个非线性映射得到历史词语的隐层表示：<br>$$g=Utanh(\theta^Tx + b_1) + Wx + b_2$$<br>其中，x为所有词语的词向量连接成的大向量，表示文本历史特征；θ、U、$b_1$、$b_2$和W分别为词向量层到隐层连接的参数。g表示未经归一化的所有输出单词概率，$g_i$表示未经归一化的字典中第i个单词的输出概率。</li>
<li>根据softmax的定义，通过归一化$g_i$, 生成目标词$w_i$的概率为：<span>$$\\
P(w_t | w_1, &hellip;, w_{t-n+1}) = \frac{e^{g_{w_t}}}{\sum_i^{|V|} e^{g_i}}$$</span><!-- Has MathJax --></li>
<li>整个网络的损失值(cost)为多类分类交叉熵，用公式表示为<span>$$\\
J(\theta) = -\sum_{i=1}^N\sum_{c=1}^{|V|}y_k^{i}log(softmax(g_k^i))$$</span><!-- Has MathJax -->
其中$y_k^i$表示第i个样本第k类的真实标签(0或1)，softmax($g_k^i$)表示第i个样本第k类softmax输出的概率。</li>
</ul>
<h4 id="Continuous-Bag-of-Words-model-CBOW"><a href="#Continuous-Bag-of-Words-model-CBOW" class="headerlink" title="Continuous Bag-of-Words model(CBOW)"></a>Continuous Bag-of-Words model(CBOW)</h4><p>CBOW模型通过一个词的上下文（各N个词）预测当前词。当N=2时，模型如下图所示：<br><img src="http://book.paddlepaddle.org/04.word2vec/image/cbow.png" width="50%"><br>$$图3,CBOW模型$$</p>
<p>具体来说，不考虑上下文的词语输入顺序，CBOW是用上下文词语的词向量的均值来预测当前词。即：<br><span>$$\\
context = \frac{x_{t-1} + x_{t-2} + x_{t+1} + x_{t+2}}{4}$$</span><!-- Has MathJax --></p>
<p>其中$x_t$为第t个词的词向量，分类分数（score）向量 $z=U*context$，最终的分类y采用softmax，损失函数采用多类分类交叉熵。</p>
<h4 id="Skip-gram-model"><a href="#Skip-gram-model" class="headerlink" title="Skip-gram model"></a>Skip-gram model</h4><p>CBOW的好处是对上下文词语的分布在词向量上进行了平滑，去掉了噪声，因此在小数据集上很有效。而Skip-gram的方法中，用一个词预测其上下文，得到了当前词上下文的很多样本，因此可用于更大的数据集。<br><img src="http://book.paddlepaddle.org/04.word2vec/image/skipgram.png" width="50%"><br>$$图4,Skip-gram模型$$</p>
<p>如上图所示，Skip-gram模型的具体做法是，将一个词的词向量映射到2n个词的词向量（2n表示当前输入词的前后各n个词），然后分别通过softmax得到这2n个词的分类损失值之和。</p>
<h3 id="PaddlePaddle"><a href="#PaddlePaddle" class="headerlink" title="PaddlePaddle"></a>PaddlePaddle</h3><p>PaddlePaddle的开发工程师介绍，自开源后，在推广方面也是下了大工夫，比如他们做了Book、Modelsl两个项目，让更多人参与到框架的使用过程中。</p>
<h4 id="Paddle-Book"><a href="#Paddle-Book" class="headerlink" title="Paddle Book"></a>Paddle Book</h4><ul>
<li>1、6个典型任务，深入讲解深度学习模型：<ul>
<li>图像分类识别</li>
<li>词向量</li>
<li>浅层句法分析</li>
<li>推荐系统</li>
<li>情感分类</li>
<li>机器翻译</li>
</ul>
</li>
<li>2,讲原理、讲模型，交互式可直接运行</li>
<li>3,深度学习的入门教科书</li>
</ul>
<h4 id="Paddle-Models"><a href="#Paddle-Models" class="headerlink" title="Paddle Models"></a>Paddle Models</h4><p>深度学习模型仓库：<a href="https://github.com/PaddlePaddle/models" target="_blank" rel="external">https://github.com/PaddlePaddle/models</a></p>
<ul>
<li>模型多：涵盖图像分类、语音识别、自然语言处理多种任务</li>
<li>全部来自已上线的成功应用案例</li>
<li>持续开发</li>
<li>全中文文档详细讲解如何运行，如何预测文档</li>
</ul>
<h3 id="语言处理"><a href="#语言处理" class="headerlink" title="语言处理"></a>语言处理</h3><p>句子的处理，词性分析等，使用百度AI提供的<a href="http://ai.baidu.com/docs#/NLP-API/top" target="_blank" rel="external">自然语言处理</a>相关接口来实现</p>
<table>
<thead>
<tr>
<th>接口能力</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td>词法分析</td>
<td>分词、词性标注、专名识别</td>
</tr>
<tr>
<td>词向量表示</td>
<td>查询词汇的词向量，实现文本的可计算</td>
</tr>
<tr>
<td>DNN语言模型接口</td>
<td>中文DNN语言模型接口用于输出切词结果并给出每个词在句子中的概率值,判断一句话是否符合语言表达习惯。</td>
</tr>
<tr>
<td>词义相似度接口</td>
<td>输入两个词，得到两个词的相似度结果</td>
</tr>
<tr>
<td>短文本相似度接口</td>
<td>短文本相似度接口用来判断两个文本的相似度得分</td>
</tr>
</tbody>
</table>
<p>###Question</p>
<h4 id="如何判断词组的搭配合理性？"><a href="#如何判断词组的搭配合理性？" class="headerlink" title="如何判断词组的搭配合理性？"></a>如何判断词组的搭配合理性？</h4><p><em>汉字数量</em> ：<a href="https://zh.wikipedia.org/wiki/GB_18030" target="_blank" rel="external">GB 18030</a>是中华人民共和国现时最新的内码字集，总共收录70244个汉字；</p>
<p>汉语的词组搭配，就像是英文中26个字母的词组一样，只需要判断这个词组即可。<br>如何在如此多的汉字数量中，有相关组合的词组的词频，这跟英文的词组搭配相似，建立相应的词组词频数据库，用户数据库也作为一部分。</p>
<p>所以这里所说的词组的搭配正确性，都是相对的正确性。汉语词组从语境中独立出来看，没有绝对的错误。所以讨论就应该是这个词在当前句子中的正确性。</p>
<h4 id="学习过程"><a href="#学习过程" class="headerlink" title="学习过程"></a>学习过程</h4><ul>
<li>1、在现有正确的词库中检索该词；</li>
<li>2、未找到的情况下给出该词的相近词；</li>
</ul>
<p>要想知道该处的”废除“用错了，我们就需要知道”正确的“使用方式。关于新华社新闻正文中包含“废黜“的其他新闻稿件：</p>
<ul>
<li><a href="http://news.xinhuanet.com/local/2015-08/12/c_128117875.htm" target="_blank" rel="external">百年神交：林纾翻译托尔斯泰 2015.08.12</a></li>
<li><a href="http://news.xinhuanet.com/world/2015-12/21/c_128553447.htm" target="_blank" rel="external">霍梅尼之孙会成为伊朗最高领袖吗？2015.12.21</a></li>
<li><a href="https://mp.weixin.qq.com/s?__biz=MzA4NDI3NjcyNA==&amp;mid=400871027&amp;idx=3&amp;sn=8f110c6bc4394e716f1e2c48be9971fd&amp;mpshare=1&amp;scene=24&amp;srcid=0802d9so2loEoBk23kk5v4sR&amp;key=a36d146816aabd34fd94d864a8cd6f2dbbd58db55131275b4481d435898ae4e474e406f2d14cca16ac28f62c53981986cba072031d7576e4c8b85b9108a056acb983b19ff2bb1f264340ef05eccda222&amp;ascene=0&amp;uin=ODM5NjUzMDQw&amp;devicetype=iMac+MacBookAir7%2C2+OSX+OSX+10.12.5&amp;version=12020810&amp;nettype=WIFI&amp;fontScale=100&amp;pass_ticket=VrYkzYs1%2FsXNiC645W8jFZbSeaFMxxanmYplb%2FkUW1kVEigmYXwIySZ4n99lJQss" target="_blank" rel="external">黄金”多多”，海昏侯缘何如此”壕” 2015.12.26</a></li>
<li><a href="http://news.xinhuanet.com/world/2015-12/30/c_128579120.htm" target="_blank" rel="external">卡塔尔退位元首度假期间摔伤腿 急送瑞士手术 2015.12.30</a></li>
<li><a href="http://news.xinhuanet.com/world/2016-05/11/c_128974778.htm" target="_blank" rel="external">巴西总统弹劾案上演“360度转折” 2016.05.11</a></li>
<li><a href="http://news.xinhuanet.com/world/2017-01/09/c_1120275986.htm" target="_blank" rel="external">新闻人物：伊朗前总统拉夫桑贾尼 2017.01.09</a></li>
</ul>
<p>同时，也发现因为历史原因，”废黜“与”废除“用错的情况：<a href="http://news.xinhuanet.com/overseas/2017-05/13/c_129604002.htm" target="_blank" rel="external">纽约议员提决议案 将排华法案签署日定为“包容日”</a>中：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#12288;&#20915;&#35758;&#26696;&#25351;&#20986;&#65292;&#23613;&#31649;&#12298;&#25490;&#21326;&#27861;&#26696;&#12299;&#24050;&#34987;&#24223;&#40668;&#65292;&#32852;&#37030;&#25919;&#24220;&#20063;&#22240;&#27492;&#36947;&#27465;&#65292;&#20294;&#21326;&#35028;&#36973;&#21463;&#30340;&#20260;&#23475;&#27704;&#36828;&#26080;&#27861;&#25273;&#21435;&#65292;&#36825;&#27573;&#25490;&#26021;&#12289;&#27495;&#35270;&#31227;&#27665;&#30340;&#21382;&#21490;&#20063;&#19981;&#24212;&#34987;&#24536;&#35760;&#12290;</span><br></pre></td></tr></table></figure></p>
<p>显然法案的弃用，应该使用”废除“。这样错误的稿件就不在学习范围之内了。</p>
<p>上述稿件使用新华网提供的<a href="http://so.news.cn/?keyWordAll=%E5%BA%9F%E9%BB%9C&amp;keyWordOne=%E5%BA%9F%E9%BB%9C&amp;keyWordIg=%E6%B2%99%E7%89%B9&amp;searchFields=0&amp;sortField=0&amp;url=xinhuanet.com&amp;senSearch=1&amp;lang=cn#search/0/废黜/1/" target="_blank" rel="external">搜索工具</a>获得。</p>
<h3 id="交互流程"><a href="#交互流程" class="headerlink" title="交互流程"></a>交互流程</h3><p>与英文不同，中文没有空格作为间隔，判断的前提应该是拿到光标符号截止，上一个符号结束的整句话，先解析其词组部分，然后再分词查询。由此下来，中文错别字的实时检查，耗时应该不会太短，接口的耗时优化会成为重要的一项工作内容。</p>
<p>在web方面，错别字检查服务需要建立在长连接之上，用户在每一步输入结束、打开文档的同时，开始错别字的实时校验。最小的校验单元可以成为一个句子。<br>先为此准备实时分析并给出相关词的准备工作。</p>
<p>交互流程如下</p>
<div id="flowchart-0" class="flow-chart"></div>

<p>端、Client、Service</p>
<div id="sequence-0"></div>

<p>详细的交互内容至少包括：</p>
<ul>
<li>客户端给出一篇文章稿件，按照分级，指出其中的不合理之处；</li>
<li>客户端给出端，按照分级，指出其中的不合理之处；</li>
<li>客户端实时编辑，通过前端获取输入完毕的一句话，按照分级，给出不合理指出，修改意见等；</li>
</ul>
<p>web:</p>
<ul>
<li>与服务器建立长连接，将用户新创建的文本补充更新到数据表；</li>
<li>开启单独的长连接服务，获取到用户正在输入的词，或者行，检查给出相近的词；</li>
</ul>
<h3 id="覆盖的端"><a href="#覆盖的端" class="headerlink" title="覆盖的端"></a>覆盖的端</h3><p>要想完成错别字消灭这件事情，就要让不同的端，覆盖到不同编辑发布的平台。web端，浏览器，会是有限考虑的一部分。</p>
<h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><ul>
<li><a href="https://zh.wikipedia.org/wiki/%E6%B1%89%E5%AD%97" target="_blank" rel="external">汉字</a></li>
<li><a href="http://edu.sina.com.cn/l/2008-11-14/0959156804.shtml" target="_blank" rel="external">教育部国家语委发布2007中国语言生活状况报告</a></li>
<li><a href="https://en.wikipedia.org/wiki/One-hot" target="_blank" rel="external">One-hot</a></li>
<li><a href="http://book.paddlepaddle.org/04.word2vec/index.cn.html" target="_blank" rel="external">PaddlePaddle-深度学习入门</a></li>
<li><a href="http://www.ruanyifeng.com/blog/2013/03/cosine_similarity.html" target="_blank" rel="external">余弦相似性:余弦值越接近1，就表明夹角越接近0度，也就是两个向量越相似</a></li>
<li><a href="http://pp1230.github.io/2015/12/13/%E8%AF%8D%E5%90%91%E9%87%8F%E5%92%8C%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B.html" target="_blank" rel="external">词向量和语言模型</a></li>
<li><a href="http://licstar.net/archives/328" target="_blank" rel="external">Deep Learning in NLP （一）词向量和语言模型</a></li>
<li><a href="http://cs224d.stanford.edu/lectures/CS224d-Lecture2.pdf" target="_blank" rel="external">Deep Learning for Natural Language Processing Lecture2: Word Vectors </a></li>
<li><a href="https://www.ibm.com/developerworks/cn/data/library/bd-1404optionanalysis/index.html" target="_blank" rel="external">基于大数据的中文舆情分析</a></li>
</ul>
<p>使用的流程图、序列图、数学公式来自hexo博客开源插件，非常感谢~</p>
<ul>
<li>流程图 ：<a href="https://github.com/bubkoo/hexo-filter-flowchart" target="_blank" rel="external">https://github.com/bubkoo/hexo-filter-flowchart</a></li>
<li>时序图 ：<a href="https://github.com/bubkoo/hexo-filter-sequence" target="_blank" rel="external">https://github.com/bubkoo/hexo-filter-sequence</a></li>
<li>hexo-math <a href="https://github.com/hexojs/hexo-math" target="_blank" rel="external">https://github.com/hexojs/hexo-math</a><br><script src="https://cdnjs.cloudflare.com/ajax/libs/raphael/2.2.7/raphael.min.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/flowchart/1.6.5/flowchart.min.js"></script><textarea id="flowchart-0-code" style="display: none">st=>start: Start
e=>end: End
op1=>operation: 分词|past
op2=>operation: 结果按等级返回|current
sub1=>subroutine: 细粒度切分
cond=>condition: 粒度划分?|approved
c2=>condition: 词频判断|rejected
io=>inputoutput: catch something...|request

st->op1(right)->cond
cond(yes, right)->c2
cond(no)->sub1(left)->op1
c2(yes)->io->e
c2(no)->op2->e</textarea><textarea id="flowchart-0-options" style="display: none">{"theme":"simple","scale":1,"line-width":2,"line-length":50,"text-margin":10,"font-size":12}</textarea><script>  var code = document.getElementById("flowchart-0-code").value;  var options = JSON.parse(decodeURIComponent(document.getElementById("flowchart-0-options").value));  var diagram = flowchart.parse(code);  diagram.drawSVG("flowchart-0", options);</script><script src="https://cdnjs.cloudflare.com/ajax/libs/webfont/1.6.27/webfontloader.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/snap.svg/0.4.1/snap.svg-min.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/underscore.js/1.8.3/underscore-min.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/js-sequence-diagrams/1.0.6/sequence-diagram-min.js"></script><textarea id="sequence-0-code" style="display: none">Browser->Client:文档、段落、句子的拆分
Client->Service: 文档、段落、实时编辑的句子
Note right of Service: 基于词向量的整句概率检查
Service->Client: 概率由高到底，作为建议词组
Client->Browser:返回等级不同的检查结果</textarea><textarea id="sequence-0-options" style="display: none">{"theme":"simple","scale":1,"line-width":2,"line-length":50,"text-margin":10,"font-size":12}</textarea><script>  var code = document.getElementById("sequence-0-code").value;  var options = JSON.parse(decodeURIComponent(document.getElementById("sequence-0-options").value));  var diagram = Diagram.parse(code);  diagram.drawSVG("sequence-0", options);</script></li>
</ul>


<!--<a href="http://yoursite.com/2017-7-27-another-chinese-words-check-solutions.html#disqus_thread" class="article-comment-link">Comments</a>
<div id="disqus_thread"></div>
<script type="text/javascript">
    /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
    var disqus_shortname = 'mickeyouyou'; // required: replace example with your forum shortname

    /* * * DON'T EDIT BELOW THIS LINE * * */
    (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
-->
<div style="display:none">
<script src="http://s4.cnzz.com/stat.php?id=1254090228&web_id=1254090228" language="JavaScript"></script>script>
</div><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->






</body>
</html>
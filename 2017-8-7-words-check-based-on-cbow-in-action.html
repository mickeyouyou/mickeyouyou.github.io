<html>
<head>
	
	<!-- hexo-inject:begin --><!-- hexo-inject:end --><title>基于CBOW模型的文章错别字识别实践</title>
	<meta name="keywords" content="fzb.me,冯宗宝,冯宗宝的blog" />

    <meta http-equiv="content-type" content="text/html; charset=utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

    
	   <link href="/css/main.css?v=3" rel="stylesheet" type="text/css" />
    
        <script src="/js/util.js" type="text/javascript"></script>
        <script>
            if(!isMobile()) {
                loadjscssfile('../css/desktop.css', 'css');
                }
        </script> 
    

    <link rel="alternate" type="application/atom+xml" href="/atom.xml" title="Atom feed">

    
	<link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=3"/><!-- hexo-inject:begin --><!-- hexo-inject:end -->
    

</head>

<body>


<!-- hexo-inject:begin --><!-- hexo-inject:end --><h2 class="title">基于CBOW模型的文章错别字识别实践</h2>
<!---
<div style="text-align:center;margin-top: -10px;">
<div class="article-category">
发表于2017年8月7日




 </div>
--->
</div>

<ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#基于条件概率的语言模型"><span class="toc-text">基于条件概率的语言模型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Continuous-Bag-of-Words-model-CBOW"><span class="toc-text">Continuous Bag-of-Words model(CBOW)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#基于PaddlePaddle的学习过程"><span class="toc-text">基于PaddlePaddle的学习过程</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#数据准备"><span class="toc-text">数据准备</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#切词处理"><span class="toc-text">切词处理</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#训练过程"><span class="toc-text">训练过程</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#模型配置"><span class="toc-text">模型配置</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Cost"><span class="toc-text">Cost</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#创建参数与Trainer"><span class="toc-text">创建参数与Trainer</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#开始训练"><span class="toc-text">开始训练</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#参考"><span class="toc-text">参考</span></a></li></ol>
<blockquote>
<p>You Shall Know a Word By the Company It Keeps.</p>
</blockquote>
<p>一个词的具体含义与上下文、语料库有关系。</p>
<h3 id="基于条件概率的语言模型"><a href="#基于条件概率的语言模型" class="headerlink" title="基于条件概率的语言模型"></a>基于条件概率的语言模型</h3><p>对于一句话的目标概率$P(w_1, …, w_T)$，如果假设文本中每个词都是相互独立的，则整句话的联合概率可以表示为其中所有词语条件概率的乘积，即：<br><span>$$\\
P(w_1, &hellip;, w_T) = \prod_{t=1}^TP(w_t)$$</span><!-- Has MathJax --><br>然而我们知道语句中的每个词出现的概率都与其前面的词紧密相关, 所以实际上通常用条件概率表示语言模型：<br><span>$$\\
P(w_1, &hellip;, w_T) = \prod_{t=1}^TP(w_t | w_1, &hellip; , w_{t-1})$$</span><!-- Has MathJax --></p>
<h3 id="Continuous-Bag-of-Words-model-CBOW"><a href="#Continuous-Bag-of-Words-model-CBOW" class="headerlink" title="Continuous Bag-of-Words model(CBOW)"></a>Continuous Bag-of-Words model(CBOW)</h3><p>CBOW模型通过一个词的上下文（各N个词）预测当前词。当N=2时，模型如下图所示：<br><img src="http://book.paddlepaddle.org/04.word2vec/image/cbow.png" width="50%"><br>$$图3,CBOW模型$$</p>
<p>具体来说，不考虑上下文的词语输入顺序，CBOW是用上下文词语的词向量的均值来预测当前词。即：<br><span>$$\\
context = \frac{x_{t-1} + x_{t-2} + x_{t+1} + x_{t+2}}{4}$$</span><!-- Has MathJax --></p>
<p>其中$x_t$为第t个词的词向量，分类分数（score）向量 $z=U*context$，最终的分类y采用softmax，损失函数采用多类分类交叉熵。</p>
<h3 id="基于PaddlePaddle的学习过程"><a href="#基于PaddlePaddle的学习过程" class="headerlink" title="基于PaddlePaddle的学习过程"></a>基于PaddlePaddle的学习过程</h3><h4 id="数据准备"><a href="#数据准备" class="headerlink" title="数据准备"></a>数据准备</h4><p>以下语料来自新华社官方稿件，均为包含“废黜”或者“废除”的文章。选用5-GRAM神经网络的方法，将一下素材做切割处理：</p>
<ul>
<li><a href="http://news.xinhuanet.com/world/2017-01/09/c_1120275986.htm" target="_blank" rel="external">新闻人物：伊朗前总统拉夫桑贾尼 2017.01.09</a></li>
<li><a href="http://news.xinhuanet.com/local/2015-08/12/c_128117875.htm" target="_blank" rel="external">百年神交：林纾翻译托尔斯泰 2015.08.12</a></li>
<li><a href="http://news.xinhuanet.com/world/2015-12/21/c_128553447.htm" target="_blank" rel="external">霍梅尼之孙会成为伊朗最高领袖吗？2015.12.21</a></li>
<li><a href="https://mp.weixin.qq.com/s?__biz=MzA4NDI3NjcyNA==&amp;mid=400871027&amp;idx=3&amp;sn=8f110c6bc4394e716f1e2c48be9971fd&amp;mpshare=1&amp;scene=24&amp;srcid=0802d9so2loEoBk23kk5v4sR&amp;key=a36d146816aabd34fd94d864a8cd6f2dbbd58db55131275b4481d435898ae4e474e406f2d14cca16ac28f62c53981986cba072031d7576e4c8b85b9108a056acb983b19ff2bb1f264340ef05eccda222&amp;ascene=0&amp;uin=ODM5NjUzMDQw&amp;devicetype=iMac+MacBookAir7%2C2+OSX+OSX+10.12.5&amp;version=12020810&amp;nettype=WIFI&amp;fontScale=100&amp;pass_ticket=VrYkzYs1%2FsXNiC645W8jFZbSeaFMxxanmYplb%2FkUW1kVEigmYXwIySZ4n99lJQss" target="_blank" rel="external">黄金”多多”，海昏侯缘何如此”壕” 2015.12.26</a></li>
<li><a href="http://news.xinhuanet.com/world/2015-12/30/c_128579120.htm" target="_blank" rel="external">卡塔尔退位元首度假期间摔伤腿 急送瑞士手术 2015.12.30</a></li>
<li><a href="http://news.xinhuanet.com/world/2016-05/11/c_128974778.htm" target="_blank" rel="external">巴西总统弹劾案上演“360度转折” 2016.05.11</a></li>
</ul>
<h4 id="切词处理"><a href="#切词处理" class="headerlink" title="切词处理"></a>切词处理</h4><p>处理过程：</p>
<ul>
<li>按句号分割成句；</li>
<li>检查空句；</li>
<li>按句分词；</li>
<li>过滤空格，逗号，顿号，冒号，双引号</li>
<li>按句分行放回处理文件</li>
</ul>
<p>接口的处理过称是通过词法分析接口完成的，最多接受32768个汉字，但需要注意文章中的特殊符号。<br>以第一篇文稿为例展示切词前后的对比：<br>处理前<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#26032;&#38395;&#20154;&#29289;&#65306;&#20234;&#26391;&#21069;&#24635;&#32479;&#25289;&#22827;&#26705;&#36158;&#23612; 2017&#24180;01&#26376;09&#26085; 21:48:33  &#26469;&#28304;&#65306; &#26032;&#21326;&#31038; &#26032;&#21326;&#31038;&#24503;&#40657;&#20848;&#65297;&#26376;&#65305;&#26085;&#30005;&#12288;&#39064;&#65306;&#20234;&#26391;&#21069;&#24635;&#32479;&#25289;&#22827;&#26705;&#36158;&#23612; &#26032;&#21326;&#31038;&#35760;&#32773;&#31302;&#19996; &#24403;&#22320;&#26102;&#38388;&#65297;&#26376;&#65304;&#26085;&#19979;&#21320;&#65292;&#20234;&#26391;&#21069;&#24635;&#32479;&#38463;&#20811;&#24052;&#23572;&#183;&#21704;&#20160;&#31859;&#183;&#25289;&#22827;&#26705;&#36158;&#23612;&#22240;&#30149;&#22312;&#24503;&#40657;&#20848;&#21435;&#19990;&#65292;&#20139;&#24180;&#65304;&#65298;&#23681;&#12290;&#25289;&#22827;&#26705;&#36158;&#23612;&#26159;&#20234;&#26391;&#25919;&#22363;&#26368;&#26377;&#24433;&#21709;&#21147;&#30340;&#20154;&#29289;&#20043;&#19968;&#12290;&#20182;&#65297;&#65305;&#65299;&#65300;&#24180;&#65304;&#26376;&#20986;&#29983;&#20110;&#20234;&#26391;&#20811;&#23572;&#26364;&#30465;&#19968;&#20010;&#24178;&#26524;&#31181;&#26893;&#20892;&#22330;&#20027;&#23478;&#24237;&#65292;&#26089;&#24180;&#36861;&#38543;&#20234;&#26391;&#20234;&#26031;&#20848;&#38761;&#21629;&#39046;&#23548;&#32773;&#12289;&#26368;&#39640;&#39046;&#34966;&#38669;&#26757;&#23612;&#12290;&#65297;&#65305;&#65304;&#65296;&#24180;&#65292;&#25289;&#22827;&#26705;&#36158;&#23612;&#25285;&#20219;&#20234;&#26391;&#20234;&#26031;&#20848;&#38761;&#21629;&#32988;&#21033;&#21518;&#39318;&#20301;&#35758;&#20250;&#35758;&#38271;&#65292;&#24182;&#36830;&#20219;&#33267;&#65297;&#65305;&#65304;&#65305;&#24180;&#12290;&#65297;&#65305;&#65304;&#65305;&#33267;&#65297;&#65305;&#65305;&#65303;&#24180;&#65292;&#25289;&#22827;&#26705;&#36158;&#23612;&#36830;&#32493;&#20004;&#23626;&#25285;&#20219;&#20234;&#26391;&#24635;&#32479;&#12290;&#21368;&#20219;&#21518;&#65292;&#25289;&#22827;&#26705;&#36158;&#23612;&#19968;&#30452;&#25285;&#20219;&#20234;&#26391;&#30830;&#23450;&#22269;&#23478;&#21033;&#30410;&#22996;&#21592;&#20250;&#20027;&#24109;&#12290;&#20316;&#20026;&#35758;&#20250;&#19982;&#23466;&#27861;&#30417;&#25252;&#22996;&#21592;&#20250;&#20043;&#38388;&#30340;&#21327;&#35843;&#26426;&#26500;&#65292;&#30830;&#23450;&#22269;&#23478;&#21033;&#30410;&#22996;&#21592;&#20250;&#26159;&#30452;&#25509;&#30001;&#20234;&#26391;&#26368;&#39640;&#39046;&#34966;&#22996;&#20219;&#30340;&#34892;&#25919;&#26426;&#26500;&#65292;&#36127;&#36131;&#35299;&#20915;&#35758;&#20250;&#21450;&#23466;&#27861;&#30417;&#30563;&#22996;&#21592;&#20250;&#20043;&#38388;&#20986;&#29616;&#30340;&#31435;&#27861;&#20914;&#31361;&#12290;&#65298;&#65296;&#65296;&#65303;&#24180;&#33267;&#65298;&#65296;&#65297;&#65297;&#24180;&#65292;&#25289;&#22827;&#26705;&#36158;&#23612;&#20219;&#20234;&#26391;&#19987;&#23478;&#20250;&#35758;&#20027;&#24109;&#12290;&#19987;&#23478;&#20250;&#35758;&#26159;&#36873;&#20030;&#20234;&#26391;&#26368;&#39640;&#39046;&#34966;&#30340;&#26368;&#39640;&#26435;&#21147;&#26426;&#26500;&#65292;&#20027;&#35201;&#32844;&#36131;&#26159;&#35752;&#35770;&#22269;&#23478;&#22823;&#20107;&#12289;&#30417;&#30563;&#39046;&#34966;&#34892;&#20026;&#12290;&#26681;&#25454;&#20234;&#26391;&#23466;&#27861;&#65292;&#19987;&#23478;&#20250;&#35758;&#21487;&#22312;&#39046;&#34966;&#19981;&#31216;&#32844;&#25110;&#22833;&#21435;&#20316;&#20026;&#39046;&#34966;&#30340;&#24517;&#35201;&#26465;&#20214;&#26102;&#24223;&#40668;&#39046;&#34966;&#12290;&#25289;&#22827;&#26705;&#36158;&#23612;&#34987;&#35748;&#20026;&#26159;&#20234;&#26391;&#25919;&#22363;&#30340;&#8220;&#28201;&#21644;&#27966;&#8221;&#25110;&#8220;&#21153;&#23454;&#20445;&#23432;&#27966;&#8221;&#65292;&#20182;&#25903;&#25345;&#22269;&#20869;&#33258;&#30001;&#24066;&#22330;&#22320;&#20301;&#65292;&#36190;&#25104;&#22269;&#26377;&#20135;&#19994;&#31169;&#26377;&#21270;&#65292;&#21448;&#22312;&#22269;&#38469;&#19978;&#20445;&#25345;&#28201;&#21644;&#24418;&#35937;&#12290;&#25289;&#22827;&#26705;&#36158;&#23612;&#37325;&#35270;&#21457;&#23637;&#23545;&#21326;&#21451;&#22909;&#20851;&#31995;&#65292;&#26366;&#20110;&#65297;&#65305;&#65304;&#65301;&#24180;&#21644;&#65297;&#65305;&#65305;&#65298;&#24180;&#20004;&#24230;&#26469;&#21326;&#35775;&#38382;&#12290;</span><br></pre></td></tr></table></figure></p>
<p>处理后:<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#10;&#65297;&#26376;&#65305;&#26085; &#30005; &#12288; &#39064; &#20234;&#26391; &#21069; &#24635;&#32479;&#25289;&#22827;&#26705; &#36158;&#23612; &#26032;&#21326;&#31038; &#35760;&#32773;&#31302; &#19996; &#24403;&#22320;&#26102;&#38388; &#65297;&#26376;&#65304;&#26085; &#19979;&#21320; &#20234;&#26391; &#21069; &#24635;&#32479; &#38463;&#20811;&#24052;&#23572;&#183;&#21704;&#20160;&#31859;&#183;&#25289;&#22827;&#26705;&#36158;&#23612; &#22240; &#30149; &#22312; &#24503;&#40657;&#20848; &#21435;&#19990; &#20139;&#24180; &#65304;&#65298;&#23681;&#10;&#25289;&#22827;&#26705;&#36158;&#23612; &#26159; &#20234;&#26391; &#25919;&#22363; &#26368; &#26377; &#24433;&#21709;&#21147; &#30340; &#20154;&#29289; &#20043; &#19968;&#10;&#20182; &#65297;&#65305;&#65299;&#65300;&#24180; &#65304;&#26376; &#20986;&#29983; &#20110; &#20234;&#26391; &#20811;&#23572;&#26364;&#30465; &#19968;&#20010; &#24178;&#26524; &#31181;&#26893; &#20892;&#22330;&#20027; &#23478;&#24237; &#26089;&#24180; &#36861;&#38543; &#20234;&#26391; &#20234;&#26031;&#20848; &#38761;&#21629; &#39046;&#23548;&#32773; &#26368;&#39640; &#39046;&#34966; &#38669;&#26757;&#23612;&#10;&#65297;&#65305;&#65304;&#65296;&#24180; &#25289;&#22827;&#26705;&#36158;&#23612; &#25285;&#20219; &#20234;&#26391; &#20234;&#26031;&#20848;&#38761;&#21629; &#32988;&#21033; &#21518; &#39318;&#20301; &#35758;&#20250; &#35758;&#38271; &#24182; &#36830;&#20219; &#33267; &#65297;&#65305;&#65304;&#65305;&#24180;&#10;&#65297;&#65305;&#65304;&#65305; &#33267; &#65297;&#65305;&#65305;&#65303;&#24180; &#25289;&#22827;&#26705;&#36158;&#23612; &#36830;&#32493; &#20004;&#23626; &#25285;&#20219; &#20234;&#26391; &#24635;&#32479;&#10;&#21368;&#20219; &#21518; &#25289;&#22827;&#26705;&#36158;&#23612; &#19968;&#30452; &#25285;&#20219; &#20234;&#26391; &#30830;&#23450; &#22269;&#23478; &#21033;&#30410; &#22996;&#21592;&#20250; &#20027;&#24109;&#10;&#20316;&#20026; &#35758;&#20250; &#19982; &#23466;&#27861; &#30417;&#25252; &#22996;&#21592;&#20250; &#20043;&#38388; &#30340; &#21327;&#35843; &#26426;&#26500; &#30830;&#23450; &#22269;&#23478;&#21033;&#30410;&#22996;&#21592;&#20250; &#26159; &#30452;&#25509; &#30001; &#20234;&#26391; &#26368;&#39640; &#39046;&#34966; &#22996;&#20219; &#30340; &#34892;&#25919;&#26426;&#26500; &#36127;&#36131; &#35299;&#20915; &#35758;&#20250; &#21450; &#23466;&#27861; &#30417;&#30563; &#22996;&#21592;&#20250; &#20043;&#38388; &#20986;&#29616; &#30340; &#31435;&#27861; &#20914;&#31361;&#10;&#65298;&#65296;&#65296;&#65303;&#24180; &#33267; &#65298;&#65296;&#65297;&#65297;&#24180; &#25289;&#22827;&#26705;&#36158;&#23612; &#20219; &#20234;&#26391; &#19987;&#23478; &#20250;&#35758; &#20027;&#24109;&#10;&#19987;&#23478; &#20250;&#35758; &#26159; &#36873;&#20030; &#20234;&#26391; &#26368;&#39640; &#39046;&#34966; &#30340; &#26368;&#39640; &#26435;&#21147; &#26426;&#26500; &#20027;&#35201; &#32844;&#36131; &#26159; &#35752;&#35770; &#22269;&#23478;&#22823;&#20107; &#30417;&#30563; &#39046;&#34966; &#34892;&#20026;&#10;&#26681;&#25454; &#20234;&#26391; &#23466;&#27861; &#19987;&#23478; &#20250;&#35758; &#21487; &#22312; &#39046;&#34966; &#19981; &#31216;&#32844; &#25110; &#22833;&#21435; &#20316;&#20026; &#39046;&#34966; &#30340; &#24517;&#35201;&#26465;&#20214; &#26102; &#24223;&#40668; &#39046;&#34966;&#10;&#25289;&#22827;&#26705;&#36158;&#23612; &#34987; &#35748;&#20026; &#26159; &#20234;&#26391; &#25919;&#22363; &#30340; &#28201;&#21644;&#27966; &#25110; &#21153;&#23454; &#20445;&#23432;&#27966; &#20182; &#25903;&#25345; &#22269;&#20869; &#33258;&#30001;&#24066;&#22330; &#22320;&#20301; &#36190;&#25104; &#22269;&#26377; &#20135;&#19994; &#31169;&#26377;&#21270; &#21448; &#22312; &#22269;&#38469; &#19978; &#20445;&#25345; &#28201;&#21644; &#24418;&#35937;&#10;&#25289;&#22827;&#26705;&#36158;&#23612; &#37325;&#35270; &#21457;&#23637; &#23545; &#21326; &#21451;&#22909;&#20851;&#31995; &#26366; &#20110; &#65297;&#65305;&#65304;&#65301;&#24180; &#21644; &#65297;&#65305;&#65305;&#65298;&#24180; &#20004;&#24230; &#26469;&#21326; &#35775;&#38382;</span><br></pre></td></tr></table></figure></p>
<h4 id="训练过程"><a href="#训练过程" class="headerlink" title="训练过程"></a>训练过程</h4><p>reader_creator<br>将词从文件中读取出来，转换成词ID，特殊的词ID：</p>
<ul>
<li><code>&lt;s&gt;</code> start </li>
<li><code>&lt;e&gt;</code> end </li>
<li><code>&lt;unk&gt;</code> unknown key 未在词典中出现的词</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">words = []</span><br><span class="line">word_limit = word_dict_limit + <span class="number">2</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> xrange(window_size):</span><br><span class="line">    words.append(paddle.layer.data(name=<span class="string">'word_%d'</span> % i, type=paddle.data_type.integer_value(word_limit)))</span><br></pre></td></tr></table></figure>
<p>数据举例：上文中第一句话将被处理为如下结构传递给PaddlePaddle作为训练数据:<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#60;s&#62;&#26032;&#38395; &#20154;&#29289; &#20234;&#26391; &#21069; &#24635;&#32479;&#10;&#20154;&#29289; &#20234;&#26391; &#21069; &#24635;&#32479; &#25289;&#22827;&#26705;&#36158;&#23612;&#10;&#20234;&#26391; &#21069; &#24635;&#32479; &#25289;&#22827;&#26705;&#36158;&#23612; 2017&#24180;01&#26376;09&#26085;&#10;&#21069; &#24635;&#32479; &#25289;&#22827;&#26705;&#36158;&#23612; 2017&#24180;01&#26376;09&#26085; 21:48:33&#10;&#24635;&#32479; &#25289;&#22827;&#26705;&#36158;&#23612; 2017&#24180;01&#26376;09&#26085; 21:48:33 &#26469;&#28304;&#10;&#25289;&#22827;&#26705;&#36158;&#23612; 2017&#24180;01&#26376;09&#26085; 21:48:33 &#26469;&#28304; &#26032;&#21326;&#31038;&#10;2017&#24180;01&#26376;09&#26085; 21:48:33 &#26469;&#28304; &#26032;&#21326;&#31038; &#26032;&#21326;&#31038;&#10;21:48:33 &#26469;&#28304; &#26032;&#21326;&#31038; &#26032;&#21326;&#31038; &#24503;&#40657;&#20848;&#60;e&#62;</span><br></pre></td></tr></table></figure></p>
<h4 id="模型配置"><a href="#模型配置" class="headerlink" title="模型配置"></a>模型配置</h4><p>词ID转化为词向量，因为词向量的训练打开稀疏训练，加快训练效率<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">embs = []</span><br><span class="line"><span class="keyword">for</span> w <span class="keyword">in</span> words[:window_size / <span class="number">2</span>] + words[-window_size / <span class="number">2</span> + <span class="number">1</span>:]:</span><br><span class="line">    embs.append(</span><br><span class="line">        paddle.layer.embedding(input=w, size=emb_size, param_attr=</span><br><span class="line">        paddle.attr.Param(name=<span class="string">'emb'</span>, sparse_update=<span class="keyword">True</span>)))</span><br></pre></td></tr></table></figure></p>
<h4 id="Cost"><a href="#Cost" class="headerlink" title="Cost"></a>Cost</h4><p>Cost为预测值与中检测的分类误差，网络拓扑结构由每一层的变量表示，以此完成神经网络的配置。</p>
<h4 id="创建参数与Trainer"><a href="#创建参数与Trainer" class="headerlink" title="创建参数与Trainer"></a>创建参数与Trainer</h4><p>Paddle的参数与拓扑结构是分离的，可以根据一个拓扑结构创建起所有参数，再使用Trainer优化对应的参数和拓扑结构。<br>创建参数parameters</p>
<ul>
<li>parameters是参数名到参数值的字典</li>
<li>创建参数时可以接受多个拓扑结构</li>
</ul>
<p>创建trainer</p>
<ul>
<li>选择优化的拓扑结构和参数</li>
<li>设置优化参数 自适应学习效率优化算法：AdaGrad, learning rate ($10^{-3}$)</li>
</ul>
<h4 id="开始训练"><a href="#开始训练" class="headerlink" title="开始训练"></a>开始训练</h4><ul>
<li>通过reader_creator 获取数据;</li>
<li>设置训练次数（num_passes）;</li>
<li>设置训练事件的响应函数：<ul>
<li>每100个Mini-batch,打印日志；</li>
<li>每轮结束，打印测试集合的cost和错误率</li>
<li>每轮结束，保存参数</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">trainer.train(</span><br><span class="line">    paddle.batch(</span><br><span class="line">        paddle.reader.buffered(</span><br><span class="line">            reader_creator(window_size=window_size,word_limit=word_dict_limit,path=<span class="string">"./newsdata/processed"</span>), <span class="number">16</span> * cpu_num * <span class="number">4000</span>),batch_size_per_cpu*cpu_num),</span><br><span class="line">    num_passes=num_passes,</span><br><span class="line">    event_handler=event_handler,</span><br><span class="line">    feeding=[w.name <span class="keyword">for</span> w <span class="keyword">in</span> words])</span><br></pre></td></tr></table></figure>
<h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><ul>
<li><a href="http://doc.paddlepaddle.org/" target="_blank" rel="external">PaddlePaddle 文档</a></li>
<li><a href="http://www.infoq.com/cn/news/2016/10/baidu-paddlepaddle-search-engine" target="_blank" rel="external">百度PaddlePaddle深度学习框架和搜索引擎基础架构</a> 文章中本身就包含错别字：PaddlePaddle 实现时的一些思考段落中，矩阵“乘发” =&gt;乘法</li>
<li><a href="http://bit.baidu.com/course/detail/id/175/column/7.html" target="_blank" rel="external">PaddlePaddle之词向量</a></li>
</ul>


<!--<a href="http://yoursite.com/2017-8-7-words-check-based-on-cbow-in-action.html#disqus_thread" class="article-comment-link">Comments</a>
<div id="disqus_thread"></div>
<script type="text/javascript">
    /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
    var disqus_shortname = 'mickeyouyou'; // required: replace example with your forum shortname

    /* * * DON'T EDIT BELOW THIS LINE * * */
    (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
-->
<div style="display:none">
<script src="http://s4.cnzz.com/stat.php?id=1254090228&web_id=1254090228" language="JavaScript"></script>script>
</div><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->






</body>
</html>